{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7e2o4WzTk5H"
      },
      "source": [
        "#**Семинар 4. Visual attention**\n",
        "\n",
        "**План занятия:**\n",
        "\n",
        "Адаптация трансформера к задаче классификации изображений. Реализация simple ViT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6BClQ60woy8"
      },
      "source": [
        "Стоит понимать базовый процесс обучения модели.\n",
        "https://pytorch-lightning.readthedocs.io/en/latest/levels/core_skills.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEKno1ZyvNue"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJWWBnPyV9oe"
      },
      "source": [
        "## Visual Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qlLVK5QWDTk"
      },
      "source": [
        "[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIDDnaMVYIIW"
      },
      "source": [
        "### Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fnF1D0GsLVx"
      },
      "source": [
        "**Вопрос:** Как можно дать модели информацию о последовательности?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDnWNEONuGQT"
      },
      "source": [
        "**Критерии энкодинга:**\n",
        "\n",
        "1) Уникальное кодирование для каждого слова\n",
        "\n",
        "2) Не должно быть разницы в дельтах между разными по длинне последовательностями\n",
        "\n",
        "3) Обобщение на длинные предложения -> bounded значения\n",
        "\n",
        "4) Детерминированность\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xPVx0TebJpT"
      },
      "source": [
        "Origin - [attention is all you need, part 3.5](https://arxiv.org/pdf/1706.03762.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "7V59fSFihj9r",
        "outputId": "a4aa3326-155d-49c1-bb3d-6f162a668d0e"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0bd74f9c1861>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    PE(x,2i) = sin(x/10000^(2i/D))\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "PE(x,2i) = sin(x/10000^(2i/D))\n",
        "PE(x,2i+1) = cos(x/10000^(2i/D))\n",
        "\n",
        "Where:\n",
        "x is a point in 1d space\n",
        "i is an integer in [0, D/2), where D is the size of the ch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKconZJlYNMi"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1Xdq4ap-eSHjgRnz08KK4UWOmrXSOdOwY)\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1-DrPfHnk1fln_sGN6THRdEDDtSOD9dy1)\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1KKdJVRnSswPi9xOGRZK8IvTBsK5Xw_aN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRTTIC0hsvi9"
      },
      "source": [
        "\"We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.\"\n",
        "\n",
        "[proof-Relative Positioning](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nACGMB80vENZ"
      },
      "source": [
        "[more examples](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P04cv-YDw5u-"
      },
      "source": [
        "Разделают понятия absolute positional encoding (APE) и relative positional encoding (RPE) - [paper](https://paperswithcode.com/method/relative-position-encodings)\n",
        "\n",
        "[Code](https://github.com/gazelle93/Transformer-Various-Positional-Encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqFAKuKXa4bX"
      },
      "source": [
        "**Positional encoding в ViT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLL2yxuE4WCW"
      },
      "source": [
        "**Задача:** реализуйте positional_encoding_1d."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdeOES8WMqV1"
      },
      "outputs": [],
      "source": [
        "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5BnBdEl4d0q"
      },
      "outputs": [],
      "source": [
        "# Дано:\n",
        "# positional_encoding: [1, seq_length, num_dim_to_encode]\n",
        "# _2i: [num_dim_to_encode//2]\n",
        "# position: [seq_length, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9wT5zYt5zm-"
      },
      "source": [
        "### ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K00xjB46HqL"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1J5TvycDPs8pzfvlXvtO5MCFBy64yp9Fa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WTBdjc2I8fKR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz_eLGnQ8lLI"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
        "                 dropout=0.):\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-5xvhVC8z-z"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, attn_dropout=0., proj_dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = 1./dim**0.5\n",
        "\n",
        "        self.qkv = ...\n",
        "        self.attn_dropout = nn.Dropout(attn_dropout)\n",
        "        self.out = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        ...\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOOF2hOM8yFH"
      },
      "outputs": [],
      "source": [
        "class ImgPatches(nn.Module):\n",
        "    def __init__(self, in_ch=3, embed_dim=768, patch_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('c h w -> c (h w)')           \n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        patches = self.patch_embed(img)\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpLgxMIW8vn1"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXB54A7U8t1n"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, depth, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFd83yMh8qJP"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_ch=3, num_classes=1000,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4,\n",
        "                 drop_rate=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQGBEUTZy2Hw"
      },
      "source": [
        "## Тренировка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OESJyO6T7dk2"
      },
      "outputs": [],
      "source": [
        "# conda create --name lec5 python=3.9\n",
        "# conda activate lec5\n",
        "# pip install --quiet \"setuptools==59.5.0\" \"pytorch-lightning>=1.4\" \"matplotlib\" \"torch>=1.8\" \"ipython[notebook]\" \"torchmetrics>=0.7\" \"torchvision\" \"seaborn\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9Y9uiR99eyH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
