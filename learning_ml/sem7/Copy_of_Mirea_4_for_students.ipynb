{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfCxjGV5oTiJ"
      },
      "source": [
        "# Vit Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "D-FMYv-jwLiV"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbFQzHV6k59K"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "3hL_6WoCwLiX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GVgNwDyzwLiY",
        "outputId": "e17b860e-2656-4ee2-bab7-9c96cf50d525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10])\n",
            "tensor([[-1.2294,  0.9040, -0.1200,  1.0714,  0.5192, -1.2210, -0.0639, -0.2710,\n",
            "          2.3938,  0.7388],\n",
            "        [ 1.4390,  1.0446,  0.9967,  2.4745, -1.3781,  0.4461,  1.5490,  0.3204,\n",
            "          0.8215, -1.1460],\n",
            "        [ 2.1096,  0.3937,  1.6926, -1.3716, -0.5218, -0.9493,  1.7596, -0.7344,\n",
            "          0.0625, -0.2907],\n",
            "        [-0.9791, -0.3166,  0.5805, -2.3791,  0.3706, -0.5217,  1.9911, -1.1256,\n",
            "          1.0570,  0.5330],\n",
            "        [-1.2397,  1.5370, -0.1331, -0.5572, -0.0576, -0.9227,  0.5054,  0.9964,\n",
            "         -0.7011,  0.3662]])\n"
          ]
        }
      ],
      "source": [
        "# Смоделируем данные\n",
        "\n",
        "n_features = 10  # Количество признаков\n",
        "n_classes = 3  # Количество классов\n",
        "batch_size = 5\n",
        "\n",
        "data = torch.randn((batch_size, n_features))\n",
        "print(data.shape)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKVgvWaawLiZ"
      },
      "outputs": [],
      "source": [
        "# Зададим простую модель\n",
        "model = nn.Linear(n_features, n_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyfujdNcwLia",
        "outputId": "69aeff39-f7f4-4956-e5d4-652d7d10f290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[-0.1430, -0.1978, -0.2459],\n",
            "        [ 0.0568,  0.2415,  1.2328],\n",
            "        [ 0.3432,  0.4882, -0.8710],\n",
            "        [-0.0182,  0.0482,  0.2527],\n",
            "        [ 0.3254, -0.0420,  0.0331]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ],
      "source": [
        "# Применим модель к вектору\n",
        "answer = model(data)\n",
        "print(answer.shape)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCsGwzuRwLib"
      },
      "outputs": [],
      "source": [
        "# Модель как наследник nn.Module\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, n_features, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin = nn.Linear(n_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lin(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJvKzV5ewLic",
        "outputId": "239a06aa-d46b-47df-9ac9-5fe59fc61bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[ 0.5724,  0.0264, -0.9001],\n",
            "        [ 0.3813,  0.7960, -0.4653],\n",
            "        [ 0.2878, -0.9297, -0.8761],\n",
            "        [-0.4443, -0.0975, -0.2454],\n",
            "        [-0.1670, -0.1594,  0.1794]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ],
      "source": [
        "# Попробуем применить модель в виде класса к данным\n",
        "model = SimpleNN(n_features, n_classes)\n",
        "\n",
        "answer = model(data)\n",
        "print(answer.shape)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I-SqCkrwLid",
        "outputId": "0b99dd26-00c8-4fdf-9db9-a627599d68cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /home/yessense/PycharmProjects/scene_vae/venv/lib/python3.8/site-packages (1.5.1)\r\n",
            "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.3.1 is available.\r\n",
            "You should consider upgrading via the '/home/yessense/PycharmProjects/scene_vae/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Linear-1                 [-1, 5, 3]              33\n",
            "================================================================\n",
            "Total params: 33\n",
            "Trainable params: 33\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.00\n",
            "Params size (MB): 0.00\n",
            "Estimated Total Size (MB): 0.00\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "model = SimpleNN(n_features, n_classes).cuda()\n",
        "\n",
        "# 5, 10\n",
        "input_size = (batch_size, n_features)\n",
        "print(summary(model, input_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIBvYYBewLie"
      },
      "outputs": [],
      "source": [
        "# Модель как sequential\n",
        "model = nn.Sequential(nn.Linear(n_features, n_classes))\n",
        "\n",
        "answer = model(data)\n",
        "print(answer.shape)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBwzYlptwLif",
        "outputId": "87e96229-02eb-49ee-a608-97016ded1420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n",
            "tensor([[-0.3989, -1.1825,  0.7602],\n",
            "        [-0.0798,  0.7053, -0.0033],\n",
            "        [-0.2838, -0.0166, -0.5549],\n",
            "        [-0.1167,  0.6097, -0.4273],\n",
            "        [-0.2631,  0.2675, -0.2055]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ],
      "source": [
        "# Модель как nn.ModuleList\n",
        "\n",
        "model = nn.ModuleList([nn.Linear(n_features, n_classes)])\n",
        "\n",
        "# answer = model(data)\n",
        "# print(answer.shape)\n",
        "# print(answer)\n",
        "\n",
        "answer = model[0](data)\n",
        "print(answer.shape)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5fE17SRwLig"
      },
      "outputs": [],
      "source": [
        "# Проверим параметры модели\n",
        "class ParametersCheck(nn.Module):\n",
        "    def __init__(self, n_features, n_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lin = nn.Linear(n_features, n_classes)\n",
        "        self.seq = nn.Sequential(nn.Linear(n_features, n_classes))\n",
        "        self.module_list = nn.ModuleList([nn.Linear(n_features, n_classes)])\n",
        "        self.list_of_layers = [nn.Linear(n_features, n_classes)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzvFgyhHwLih",
        "outputId": "e2da82be-f4b1-4749-ac11-d7256e5fc026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Параметр #1.\n",
            "\ttorch.Size([3, 10])\n",
            "Параметр #2.\n",
            "\ttorch.Size([3])\n",
            "Параметр #3.\n",
            "\ttorch.Size([3, 10])\n",
            "Параметр #4.\n",
            "\ttorch.Size([3])\n",
            "Параметр #5.\n",
            "\ttorch.Size([3, 10])\n",
            "Параметр #6.\n",
            "\ttorch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "model = ParametersCheck(n_features, n_classes)\n",
        "\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    print(f'Параметр #{i + 1}.')\n",
        "    print(f'\\t{param.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9_ccpqgpwLih"
      },
      "source": [
        "## ViT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "O9Ck2xnvwLii"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?export=view&id=1J5TvycDPs8pzfvlXvtO5MCFBy64yp9Fa)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFzQd5YDEbas",
        "outputId": "2d65c935-292a-4a46-bcdc-e750d78fbb27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "khe7vy_ZwLii"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "# from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbPI9vsXDZH9"
      },
      "source": [
        "![](https://amaarora.github.io/images/vit-01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Au2Fd1FZbj"
      },
      "source": [
        "## Часть 1. Patch Embedding, CLS Token, Position Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjbKwA7lGY3O"
      },
      "source": [
        "![](https://amaarora.github.io/images/vit-02.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tH4Nb22GeuS",
        "outputId": "67c148e2-d216-4e7f-df44-983eac5e6a12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([196, 768])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# input image `B, C, H, W`\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "# 2D conv\n",
        "conv = nn.Conv2d(3, 768, 16, 16)\n",
        "conv(x).reshape(-1, 196).transpose(0,1).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WVwf4n1bwLik"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embeddings = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b c h w -> b (h w) c')\n",
        "        )\n",
        "\n",
        "        self.position_embeddings = nn.Parameter(torch.randn(1, (img_size // patch_size) ** 2 + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "\n",
        "        patches = self.patch_embeddings(x)\n",
        "        patches = torch.cat((patches, repeat(self.cls_token, '() n e -> b n e', b=x.shape[0])), dim=1)\n",
        "        patches += repeat(self.position_embeddings, '() n e -> b n e', b=x.shape[0])\n",
        "\n",
        "        return patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E57UzPBuE4qi",
        "outputId": "402db5b4-e293-4dd7-d25a-6d97772466a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "patch_embed = PatchEmbedding()\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "patch_embed(x).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVUm-TJFGm6L"
      },
      "source": [
        "![](https://amaarora.github.io/images/vit-03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUxuB53PFv1h"
      },
      "source": [
        "## Часть 2. Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkklM-fqFpa9"
      },
      "source": [
        "![](https://amaarora.github.io/images/ViT.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G34WzminccX7"
      },
      "source": [
        "![](https://amaarora.github.io/images/vit-07.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACAqbCivDGsa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VPQts2WWdeYQ"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_features = hidden_features or in_features\n",
        "        out_features = out_features or in_features\n",
        "\n",
        "        # Linear Layers\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(in_features=in_features, out_features=hidden_features),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=drop),\n",
        "            nn.Linear(in_features=hidden_features, out_features=out_features),\n",
        "            nn.Dropout(p=drop),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # Activation(s)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.nn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFxxcPoMf7IW",
        "outputId": "c1f44b38-7ec4-4e93-bdb5-a4db4e116e90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "mlp = MLP(768, 3072, 768)\n",
        "out = mlp(x)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4QnAW3rSc2OZ"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., out_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Sequential(\n",
        "            nn.Linear(in_features=dim, out_features=3*dim, bias=qkv_bias),\n",
        "            # torch.view()\n",
        "            # Rearrange('b n (k h e) -> b n k h e', h=num_heads, e=head_dim, k=3)            \n",
        "        )\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.out = ...\n",
        "        self.out_drop = nn.Dropout(out_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        (b, n, e) = x.shape\n",
        "        # Attention\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = qkv.view(b, n, 3, self.num_heads, self.head_dim)\n",
        "        print(qkv.shape)\n",
        "        q, k, v = torch.split(qkv, [1, 1, 1], dim=2)\n",
        "        print(q.shape, k.shape, v.shape)\n",
        "        # k = k.transpose(-2, -1)\n",
        "        # attention = (q @ k) * self.scale\n",
        "\n",
        "        # Out projection\n",
        "\n",
        "        ...\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_vgvLDbcjvi"
      },
      "source": [
        "![](https://amaarora.github.io/images/vit-08.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OukFkeXzdFpB"
      },
      "outputs": [],
      "source": [
        "# attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "# attn = attn.softmax(dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NeRHHJAgg5R",
        "outputId": "143fdcfe-b61b-47c5-efd2-e511010f43a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 197, 3, 12, 64])\n",
            "torch.Size([1, 197, 1, 12, 64]) torch.Size([1, 197, 1, 12, 64]) torch.Size([1, 197, 1, 12, 64])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "attention = Attention(768, 12)\n",
        "out = attention(x)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6e8y_YvwLik"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalization\n",
        "        self.norm1 = nn.LayerNorm()\n",
        "\n",
        "        # Attention\n",
        "        ...\n",
        "\n",
        "        # Dropout\n",
        "        ...\n",
        "\n",
        "        # Normalization\n",
        "        self.norm2 = nn.LayerNorm()\n",
        "\n",
        "        # MLP\n",
        "        self.MLP = MLP(768, 3072, 768)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attetnion\n",
        "        ...\n",
        "\n",
        "        # MLP\n",
        "        ...\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aMihgfEhyql",
        "outputId": "993524e8-c7eb-4b38-802e-557e89ba8285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "block = Block(768, 8)\n",
        "out = attention(x)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPBmiO5FhoN6"
      },
      "source": [
        "В оригинальной реализации теперь используется [DropPath](https://github.com/rwightman/pytorch-image-models/blob/e98c93264cde1657b188f974dc928b9d73303b18/timm/layers/drop.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1uO18VTwLil"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, depth, dim, num_heads=8, mlp_ratio=4, drop_rate=0.):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList([\n",
        "            Block(dim, num_heads, mlp_ratio, drop_rate)\n",
        "            for i in range(depth)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIfp984oiBqc",
        "outputId": "704c9a49-92d6-4859-9f61-06555bf89579"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 197, 768)\n",
        "block = Transformer(12, 768)\n",
        "out = attention(x)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqUxpyv3cwNm"
      },
      "source": [
        "![](https://amaarora.github.io/images/vit-06.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9gyxdqQeFs6"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.normalization import LayerNorm\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000,\n",
        "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "                 qkv_bias=False, drop_rate=0.,):\n",
        "        super().__init__()\n",
        "\n",
        "        # Присвоение переменных\n",
        "        ...\n",
        "\n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        ...\n",
        "\n",
        "        # Transformer Encoder\n",
        "        ...\n",
        "\n",
        "        # Classifier\n",
        "        ...\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Path Embeddings, CLS Token, Position Encoding\n",
        "        ...\n",
        "\n",
        "        # Transformer Encoder\n",
        "        ...\n",
        "\n",
        "        # Classifier\n",
        "        ...\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lGhne8kjeYs",
        "outputId": "c660e099-46aa-4700-f5f5-25c69c8d9bda"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 1000])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = torch.randn(1, 3, 224, 224)\n",
        "vit = ViT()\n",
        "out = vit(x)\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QbFtayBkp-c"
      },
      "source": [
        "# Домашнее задание"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nZbwbK9kskc"
      },
      "source": [
        "\n",
        "1. Выбрать датасет для классификации изображений с размерностью 64x64+\n",
        "2. Обучить ViT на таком датасете.\n",
        "3. Попробовать поменять размерности и посмотреть, что поменяется при обучении.\n",
        "\n",
        "\n",
        "Примечание:\n",
        "- Датасеты можно взять [тут](https://pytorch.org/vision/stable/datasets.html#built-in-datasets) или найти в другом месте.\n",
        "- Из за того, что ViT учится медленно, количество примеров в датасете можно ограничить до 1к-5к."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
